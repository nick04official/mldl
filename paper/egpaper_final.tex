% !TEX encoding = UTF-8 Unicode
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp,gensymb}
\usepackage{ae,aecompl}

\usepackage[english]{babel}
\usepackage[none]{hyphenat}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig,graphicx}
\usepackage{amsmath,amssymb}

\usepackage{enumitem}
\setlist{nosep}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false,hidelinks=true]{hyperref}

\cvprfinalcopy

%\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
%\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}

\author{Nicolò Bertozzi\\
S276406\\
{\tt\small nicolo.bertozzi@studenti.polito.it}
\and
Francesco Bianco Morghet\\
S277910\\
{\tt\small s277910@studenti.polito.it}
}

\maketitle
%\thispagestyle{empty}


\begin{abstract}
\end{abstract}


\section{Introduction}

One of the main challenges in the field of Computer Vision and Artificial Intelligence lies in the recognition of the actions included inside a video. Until now, the research has been concentrated on the data provided by specific kinds of cameras, used in particular ares and in particular places, which capture the alternating of the everyday life from an external narrator point of view, i.e in third person.

A lot of areas are interested in a classification based on first person videos. Among them, the robotic applications could be surely cited, starting from the definition of the androids’ intelligence like iCub\textsuperscript{\textcopyright} to the autonomous driving, together with the surveillance, the composition of the people’s behavioral profile, in order to predict and then to prevent potential offenses, the analysis of the users’ content informations to allow a tracking of their preferences loyalizing their experience inside the service etc.

The datasets which contains these informations, until now, are very small but, in the future with the growing increase of the sale of wearable devices, with the undeterred increment of the ease to have at our disposal a camera and with the exponential growth of the number of photos and videos taken every day in all over the world, this limit could be easily surpassed. 

In addition the first person images present further difficulties than the one in third person. First of all, it is very frequent to observe inside this videos limbs of him who is capturing the action. So this features has to be used in order to promote the action recognition and not to disadvantage the task. Then it is very probable that the camera which is taking the video is mounted on the head of the subject who is performing the operation and if so also the perspective change basing on what the cameraman is watching in that moment.

Dependently on the context, the video classification could be more precise or less precise. A general purpose recognition can be summarized by a simple verb, which defines without accuracy the action in progress. Instead a more careful recognition is composed by a verb plus a noun. In fact the difference between the task \emph{open} and \emph{open a door} and still \emph{open a jar} is completely different.

\section{Related works}

The first person action recognition task has been tackled by several works in recent years, and various solutions involving deep learning algorithms have been proposed.

In most works, a convolutional neural network is used to extract feature embeddings from RGB images; then, a Convolutional Long Short-Term Memory (ConvLSTM) is used to model the temporal dependencies between the feature maps extracted by the convolutional segment and the spatial correlations inside each of those; finally, a linear classifier on the features extracted by the ConvLSTM is used to classify the portrayed action.

However, the aforementioned model focuses on the appearances of each single frame, but it fails to take into account the motion happening in those frames in a effectively manner. To tackle this problem, usually a separate convolutional neural network is trained on optical flows images.

Then, on top of these two trained networks, a linear classifier is used to determine the class label of the given sample: while the classifier is trained on the output features of the two networks, the two networks are fine tuned on the weights obtained in a preceding set of training stages.

The main disadvantage of this two stream approach is that motion informations and appearance informations are decoupled, and so, since they are treated in two separate networks, any correlations among the two are not taken into account.

There exist however an attempts to go beyond the two streams approach and to deal with both appearance and motion in a single network by using an auxiliary task, as in the case of \cite{planamente2020joint} where a motion segmentation task is deployed.

\subsection{Attention model}
One milestone for the development of a working solution for recognising the involved action is the acknowledgement of the important role played by the motion of the hands and the appearance of the objects in the scene. So, focusing the attention on the most important region of a frame of a video, while at the same time discarding irrelevant informations, is a key component for getting a great performance gain in terms of accuracy. For this reason, in \cite{Ego-RNN} an attention mechanism is employed in the RGB network in order to extract features with spatial attention.

To extract features with spatial attention, \textit{class activation maps} (CAMs) are employed: the $l$ output feature maps of the very last convolutional layer of the backbone are first forwarded to an average pool that reduces each feature map size to a 1x1 block, then those reduced feature maps are forwarded to a fully connected layer. The CAM for class $c$ is obtained, for each spatial location $i$, by dot multiplying the values of the $l$ feature maps at location $i$ and the weights of the neuron of the fully connected layer of class $c$, i.e. ${\text{CAM}_c(i) = \sum_l w_l^c f_l(i)}$. 

So, the features with spatial attention are obtained by performing the Hadamard product between the output feature maps of the very last convolutional layer of the backbone and the output of the softmax operation applied to the CAM of the class with the highest probability.

The limit of the attention mechanism is that it almost completely discard any motion information but it only focuses on the appearance of each single frame. So, despite the deployment of the attention mechanism, in \cite{planamente2020joint} in order to reach state of the art performances, a separate convolutional neural network is trained on stacked warp flow images, and then, a linear classifier is put on top of the output features of the two networks.

\subsection{Motion segmentation task}
The main idea behind this procedure is the addition of an auxiliary task: given an architecture where first a convolutional neural network extracts feature embeddings from RGB frames and then a ConvLSTM models temporal and infra-spatial correlations of such features, a second task is added to the output of the convolutional neural network with the intention of helping this shared backbone to extract more meaningful features and taking motion into account as well.

In \cite{planamente2020joint}, a motion segmentation task is proposed. To do so, an \textit{improved dense trajectory} (IDT) is associated to each frame; basically, an IDT is a binary map that compensates for camera motion and that, for each pixel, states whether a moving keypoint is present or not. This motion map is used as a ground truth for a motion segmentation task: the output feature maps of the last convolutional layer of the shared backbone are forwarded to a second branch where they are first processed by a convolutional layer and then forwarded to a fully connected layer. This auxiliary task aims to minimise the pixel-by-pixel cross entropy loss between the predicted motion map and the ground truth. Both auxiliary task and main task tasks are trained together, because the role of the auxiliary task is to help the shared backbone to extract better features so that they take into account motion as well: for this reason, the losses of the auxiliary task and the main task are summed together during training.


\section{Proposed method}
Our method is based on the fact that the employment of optical flow has been proved to be effective to capture and encode motion cues from moving objects. In this work, the warp flow is used since, as it has been introduced in the paragraphs above, it allows for the compensation of camera movements, which is a great factor to take into account in first person videos.

Our proposed method draws its inspiration from \cite{carlucci2017de2}, where a convolutional neural network is used to colorise depth images. Our method aims to extract RGB frames from warp flow frames: the generated RGB frames can then be fed to an RGB network in order to classify the sample.

So, the employed method can be decomposed into two macro steps, which are briefly presented here and will be discussed in more details in the following paragraphs:
\begin{enumerate}
	\item A convolutional neural network, which will be referred to as WFCNet from this point on, is trained so that a mapping between warp flow frames and generated RGB frames is learnt; 
	\item Using WFCNet for inferring RGB frames from warp flow frames, a deep neural network consisting in a convolutional backbone, a ConvLSTM, an average pooling layer and a final fully connected layer is trained in a multi stage manner; we also propose a two stream variant, which consists in two backbones, one fed with colorised warp flow frames and the other with the original RGB frames, and whose output features are concatenated and fed to the ConvLSTM.
\end{enumerate}

\subsection{WFCNet}

\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=\textwidth]{schemi/WFCNet.img.pdf}		
	\end{center}
	\caption{WFCNet architecture representation}
	\label{fig:WFCNet}
\end{figure*}

WFCNet is a ResNet like network: it makes use of residual blocks which introduce additional direct connections from input to output that skip convolutional layers. This procedure allows for better gradient flow in the backward propagation step and prevents the gradient vanishing problem that limits the deepness of neural networks.

WFCNet is made of basic blocks: each block has got two convolutional layers with a kernel size of 3, a stride 1 and a padding of 1, one ReLu between them, batch normalisation at the output of each convolutional layer, and a direct connection from input to output, which is obtained by summing the input of the block to the output of the second convolution.
As suggested by the work presented in \cite{springenberg2014striving}, the downsampling of the input is done using a convolutional filter instead of a simple average pooling: in this way, an embedding can be learnt leading to better downsampled feature maps, while at the same time the advantages of the residual block are preserved.
The upsampling of the input is performed instead by applying a nearest neighbour resizing followed by a convolution filter with a stride 1 and kernel size of 1.

These basic blocks are grouped together in macro blocks. It should be noted that the downsampling or upsampling are performed only by the first convolutional layer of each macro block. The downsampling is the result of the adoption of a convolution with a stride of 2, kernel size of 3 and padding of 1; on the other hand, the upsampling is performed using a nearest neighbour resizing followed by a convolutional filter with a stride 1, kernel size of 3 and padding of 1 on every side. 

This structure with gradual downsampling followed by a gradual upsampling allows for the extraction of RGB images with semantic segmentation for hands and those objects involved in the action, and at the same time all the other still objects are discarded and melted with the background. In this work, the transposed convolution for upsampling is not employed because its introduction resulted in repeating patterns in the output RGB frames that would not disappear even after many epochs of training: the employment of nearest neighbour for upsampling instead leads to better segmented images, which in turn leads to better overall results.

Finally, the output RGB frames of WFCNet are normalised: first the Sigmoid function is applied to every output value in order to get values in the $[-1; +1]$ range; then, every channel of each pixel is normalised on the means and standard deviations of ImageNet, which are \texttt{mean=[0.485, 0.456, 0.406]}, \texttt{std=[0.229, 0.224, 0.225]}. The introduction of this block improves the performances of the model and its stability at the beginning of the training because, in the overall training procedure of this network, a ResNet-34 pretrained on ImageNet is employed. Also, since WFCNet is later employed to colorise frames which are then fed to an RGB network whose backbone is pretrained on ImageNet, it is important that those generated images are normalised in the correct manner in order to achieve optimal results. By explicitly introducing this block, we can be sure that the generated images meet this requirement.

\subsection{Training WFCNet}

The aim of the WFCNet block is to learn a mapping from input warp flow frames to output RGB frames.

We propose two variants for WFCNet, which differ in the kind of input they accept:
\begin{enumerate}
	\item One single warp flow frame, consisting in two channel, i.e. the x-axis displacement and the y-axis displacement;
	\item A stack of five consecutive warp flow frames, which account to a total number of 10 channels.
\end{enumerate}

In order to train WFCNet, a ResNet-34, followed by an average pooling and a fully connected layer, is employed. The complete architecture is represented in figure \ref{fig:WFCNet}: the input frame is colorised by WFCNet; then the resulting RGB image is fed to the ResNet-34; finally, the output features of the ResNet-34 are forwarded to the classification block, which consists in an average pooling (which reduces the feature map from a 512x7x7 volume to a vector of 512 elements) and a fully connected layer.

\begin{figure*}
	\begin{center}
		\includegraphics[width=\textwidth]{schemi/single_stream.img.pdf}		
	\end{center}
	\caption{WFCNet architecture representation}
	\label{fig:SingleStream}
\end{figure*}

Each video sample consists in possibly more than one input frame, accounting for different time steps: for this reason, for each sample, multiple input frames are forwarded to the network, multiple RGB images are generated and so multiple output scores are produced by the final fully connected layer. For this reason, the average of the output scores is computed with respect to the time dimension.

We treat this task as a classification problem, where the aim of the whole architecture (consisting in WFCNet, ResNet, and classification block) is to predict the class label of the sample. For this reason, the \textit{Cross Entropy Loss} is employed, which can be expressed as ${L = -\sum_i^N{\log(P(y_i | x_i, w))}}$, where $N$ is the total number of samples, $x_i$ is the input sample, $y_i$ is the true label, and $P(y_i | x_i, w)$ is the predicted probability for the true label $y_i$ given the sample $x_i$ and the weights of the network $w$. The normalised probabilities for the different classes are computed using the softmax function: ${P(y_i | x_i, w) = \frac{e^{f_{y_i}}}{\sum_i e^{f_i}}}$.

The whole training procedure consists in two main steps:
\begin{enumerate}
	\item The ResNet-34 is pre-trained on ImageNet and its weights are kept frozen; \cite{he2015delving}
\end{enumerate}

\begin{figure*}
	\begin{center}
		\includegraphics[width=\textwidth]{schemi/two_stream.img.pdf}		
	\end{center}
	\caption{WFCNet architecture representation}
	\label{fig:TwoStream}
\end{figure*}

\section{Experimental results}

\subsection{Ego-RNN}

\subsection{Self supervised task}

\section{Conclusions}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
